#对感知机学习的理解
感知机作为学习机器学习领域的相关内容，是所有入门者都需要了解和掌握的，以下将对感知机进行详细介绍。
介绍感知机前，先明确两个数学概念：直线的**方向向量**和**法向量**（二维平面为例）

直线的方向向量（定义）：**空间与直线平行的非零向量**
直线的法向量（定义）：**空间中与直线垂直的非零向量**


![image-20200703145922085](https://github.com/Aragron-moon/MachineLearning/blob/master/StasticticsML/picture1.png)

如上图所示，对于直线l而言 交坐标轴分别于MN两点，m向量为l的方向向量，n为l的法向量。

如果一条直线用以下方程进行表示：
Ax1 + Bx2 + C = 0 (x1和x2分别表示平面直角坐标系中的两个坐标轴)

那么该直线的方向向量是（-B，A）或（B,-A）；该直线的法向量是（A,B）或（-A,-B）

**什么是感知机**

感知机是指接受多个信号，仅输出一个信号（模拟神经元），结构如下图所示：

![image-20200703154728601](https://github.com/Aragron-moon/MachineLearning/blob/master/StasticticsML/picture2.png)

其中x1和x2是输入信号，y是输出信号，w1和w2是输入信号对应的权重，数学表达式如下：
$$
y=\begin{cases} 0,w1x1+w2x2≤θ\\1,w1x1+w2x2>θ \end{cases}
$$
当输入信号和权重进行相乘再求和后，满足某个条件（阈值θ）则会决定输出值为0还是1。换句话说，在二维平面内，感知机将确定一条直线，这条直线

将平面分成了正负两部分，直线上的为正（类别1），直线下的为负（类别0），是一个二元线性分类器。

**感知机是如何学习的**

我们在学习神经网络的时候，明白对于一个网络参数的学习是绝大多数都是通过梯度下降，不断优化我们的模型。所以对于感知机的学习而言，我们要明白

几个地方，优化的是什么？代价函数（损失函数）是什么？

对于感知机而言，要优化或者说学习的参数就是w1，w2，θ，因为这三个参数决定了平面中直线的位置，从而决定了感知机对于分类结果的好坏。

对于损失函数，采用的是误分类点在到分类超平面（二维中为一条线）的距离，此处的距离指的是点到直线的距离(||w||是L2范数，x0是某一个误分类点的特征值)：
$$
\frac{|w*x0 + b|}{||w||}
$$

那么对于误分类点（xi,yi）来说(xi为该点特征值，yi为该点真实标签值，将（w*x + b）≥ θ 分为一类 ：+1，将（w*x + b）< θ 分为一类：-1 )：
$$
-yi(w*xi + b) > 0
$$
误分类点到超平面的总距离：
$$
-\frac{\sum yi(w*xi + b)}{||w||}
$$
不考虑||w||，就得到了感知机的损失函数（之所以可以不用考虑||w||，因为w*x+b=0的系数，可以按照一定倍数缩放，并且不会影响该超平面的位置，因此可以选一个w为单位法向量的平面方程，这样就不用考虑||w||了，因为||w||=1）：
$$
L(w,b)=-\sum yi(w*xi + b)
$$
根据上述的代价函数，可以看出损失函数是非负的。如果没有误分类点，那么损失函数的值为0，而且误分类点越少，误分类点距离超平面也就越接近，损失函数值也越小。同时，L(w,b)是连续可导的。

在得到了损失函数之后，就是感知机的优化步骤。采用随机梯度下降算法对感知机的参数（w1,w2,θ）进行优化，也即是最小化L(w,b)，极小化过程不是一次使所有误分类点的梯度下降，而是一次随机的选取一个误分类点使其梯度下降。根据损失函数计算参数偏导：
$$
\frac{\varrho L(w,b)}{\varrho w} = - \sum yixi\\
\frac{\varrho L(w,b)}{\varrho b} = - \sum yi
$$
参数优化策略：
$$
\Theta = \Theta - \alpha \bigtriangledown \tau(\Theta)
$$
然后在所有误分类点中，随机选择一个点进行参数的更新（其中α是学习率，位于0到1之间），直到没有误分类点为止：
$$
w = w + \alpha yixi\\
b = b + \alpha yi
$$
**参数更新的可视化**

![1596430609(1)](https://github.com/Aragron-moon/MachineLearning/blob/master/StasticticsML/picture3.png)

上图中的w是当前学习的超平面的法向量，x是一个误分类点（方便理解 假设参数b=0，也可以把参数b考虑到w中）。因为（w*x）< 0，所以才会将其误分类，从向量的假赌来考虑，正是w和x的夹角＞90，w和x的乘积才会小于0，要纠正这一个误分类点，就必须缩小二者之间的夹角，所以要将w向x方向进行调整，采用w=w+x的方法进行调整！



**注意**

需要小心的是，我们规定了（w*x + b）≥ θ 为+1类，当然也可以规定为 -1类，不过这样后面的参数更新也要注意符号的变化，因为此时对于误分类点来说：
$$
-yi(w*xi + b) < 0
$$
因此后面的参数学习时候，符号也需要取反：
$$
w = w - \alpha yixi\\
b = b - \alpha yi
$$
这个地方就是一直困扰我的地方！好在后面终于弄明白了！